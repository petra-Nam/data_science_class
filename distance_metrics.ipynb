{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # <-- Add this line\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KNN classifier with Euclidean distance\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "\n",
    "# Train the model\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn_euclidean.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with Euclidean distance: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Euclidean distance appropriate for this dataset? \n",
    "The features of the Iris dataset (like petal length and sepal width) are continuous measurements in a real-valued space. Euclidean distance is a natural choice because it measures the actual geometric, straight-line distance between points, which is a meaningful way to determine similarity for this kind of physical data.\n",
    "\n",
    "\n",
    "### How would changing k affect your accuracy? \n",
    "Changing 'k' involves a trade-off:\n",
    "\n",
    "A small k (like 1) makes the model highly sensitive to noise and outliers. It can lead to overfitting, where the model learns the training data too well but doesn't generalize to new data.\n",
    "\n",
    "A large k makes the model smoother and less affected by individual points. However, if 'k' is too large, it can underfit, missing local patterns and becoming too simplistic. You'll explore this trade-off in Part 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
